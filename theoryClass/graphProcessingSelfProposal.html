<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Report on solution to speed up Graph Processing by improving load balancing&period;</title>
        <style>
</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        
        
        
    </head>
    <body class="vscode-body vscode-light">
        <h1 id="report-on-solution-to-speed-up-graph-processing-by-improving-load-balancing">Report on solution to speed up Graph Processing by improving load balancing.</h1>
<ul>
<li><a href="#report-on-solution-to-speed-up-graph-processing-by-improving-load-balancing">Report on solution to speed up Graph Processing by improving load balancing.</a>
<ul>
<li><a href="#problem-introduction">Problem Introduction</a></li>
<li><a href="#solution-for-load-balancing">Solution for load balancing</a>
<ul>
<li><a href="#general-idea">General idea</a></li>
<li><a href="#note-indexing">Note Indexing</a></li>
<li><a href="#sort-running-time">Sort Running Time</a></li>
<li><a href="#exchanging-vertices">Exchanging vertices</a></li>
<li><a href="#access-exchanged-vertices">Access Exchanged Vertices</a></li>
<li><a href="#support-scaling-up">Support Scaling Up</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</li>
</ul>
<h2 id="problem-introduction">Problem Introduction</h2>
<ul>
<li>To guaranteethe quality of different services while lowering maintenanceand energy cost, data centers deploy a diverse collectionof compute nodes ranging from powerful enterprise serversto networks of off-the-shelf commodity parts. Besidesrequirements on service quality, cost and energy consumption,data centers are continuously upgrading their hardware in arotating manner for high service availability. These trendslead to the modern data centers being populated with hetero-geneous computing resources. For instance, low-cost ARM-based servers are increasingly added to existing x86-basedserver farms to leverage the low energy consumption. Despite these trends, most cloud computing and graph processing frameworks, like Hadoop , and PowerGraph, are designed under the assumption that all computingunits in the cluster are homogeneous [1]
<ul>
<li><img src="file:////home/anhvu/gitRepo/myINIAD/theoryClass/imgs/bigAndSmall.png" alt="alt text"></li>
</ul>
</li>
<li>And to make the problems even harder, it is not easy to calculate the best ratio of work for each machine. As in graph processing, computation-to-communication ratio is very low [2]
<ul>
<li><img src="file:////home/anhvu/gitRepo/myINIAD/theoryClass/imgs/predict.png" alt="alt text"></li>
</ul>
</li>
<li>And also, with the rapid development of web service that provides computing capacity, nowadays, most service providers support auto scaling that allow multiply number of running instance when needed such as Amazon EC2 or Microsoft Azure. But still, most graph processing frameworks only run partitioning when loading the graph in stead of support scaling up when needed.</li>
<li>To resolve that, in this report, I propose a method that allow process improve load balancing while running, and also add the ability to scale up number of running computers/server instances when needed.</li>
</ul>
<h2 id="solution-for-load-balancing">Solution for load balancing</h2>
<h3 id="general-idea">General idea</h3>
<p>As in graph processing, computation-to-communication ratio is very low [2], we want to ultilize unuse computation capacity to sort all machines by running time each round. After that, slower machine will send a number of vertices to faster machine for next round computition.</p>
<h3 id="note-indexing">Note Indexing</h3>
<ul>
<li>Here we assume that the vertices are numbered from 0 to Nâˆ’1 by a preprocessing step. During loading, we decide how these vertices are partitioned into P machines.  All the vertices will be re-index with a 64 bits number. First 16bits show where the vertex is located, and the last 48bits shows its assigned number. This information will be shared accross all machines so that when the information of a vertice is needed, the machine can easily know where to look for it.[3]
<ul>
<li>If the number of node (N) or the number of machines (P) are too big, it is possible to index each vertex with a 128 bits number.</li>
</ul>
</li>
</ul>
<h3 id="sort-running-time">Sort Running Time</h3>
<ul>
<li>After  finished 70% computation of each round, the machine will broadcast a message to signal this event as at this time, we can predict with high accuracy how much time the machine will need to finish the round.</li>
<li>In each machine, after receive all the time to finish 70% work load from all the machines, it will sort all the machine by running time from fastest to slowest. As the information all the machines receive should be identical, the sorted list in each machine should also be identical.</li>
</ul>
<h3 id="exchanging-vertices">Exchanging vertices</h3>
<ul>
<li>
<p>After obtained the sorted running time list, the slowest machine will contact will fastest machine for exchaning node, the second slowest with the second fastest, and so on.</p>
</li>
<li>
<p>When i-th slowest machine(machine S) contact with i-th fastest machine(machine F), S will send 2 number:</p>
<ul>
<li>number of vertices that S has: <code>Ns</code></li>
<li>time in average for S to finish computation for one vertex: <code>Ts</code></li>
</ul>
</li>
<li>
<p>F after receive <code>Ns</code> and <code>Ts</code>, with <code>Nf</code> is number of vertices F has and <code>Tf</code> is time in average for F to finish computation for one vertex. F will send request that accept <code>N-exchange</code> vertices from F with:</p>
<p><code>N-exchange = (Ns x Ts - Nf x Tf) / ( Ts + Tf ) * a</code> (0.25 &lt; a &lt; 0.75. Default 0.5)</p>
</li>
<li>
<p>Finally, S send a list contain <code>N-exchange</code> re-indexed vertices to F with first 16bit show value of F instead of S. Next round, F will compute these vertices for S</p>
</li>
</ul>
<h3 id="access-exchanged-vertices">Access Exchanged Vertices</h3>
<ul>
<li>After S sends <code>N-exchange</code> vertices to F, these vertices index are not updated other than in S and F. As the result, when needed data from these vertices, other machines still send request to S in stead of F.</li>
<li>To solve this, after each round, S still store value of these vertices by copying from F. Also, in reply message, S will also includes new index of these vertices so that next time F will be asked.</li>
<li>After pre-determined number of round with no machine request for value of these vertices, S will stop copying value from F instead only keep newer index that point to machine</li>
</ul>
<h3 id="support-scaling-up">Support Scaling Up</h3>
<ul>
<li>When new machine join the process, it will immediately broadcast that it finished 70% the work load in 0ms. By this in next round, it will receive vertices from slowest machines.</li>
</ul>
<h2 id="references">References</h2>
<p>[1] Shuang Song, Meng Li,et al., &quot;Proxy-Guided Load Balancing of Graph ProcessingWorkloads on Heterogeneous Clusters&quot; p77. 2016</p>
<p>[2] Sungpack Hong, Siegfried Depner,et al., &quot;PGX.D: A Fast Distributed Graph Processing Engine&quot; p1. 2018</p>
<p>[3] Sungpack Hong, Siegfried Depner,et al., &quot;PGX.D: A Fast Distributed Graph Processing Engine&quot; p3. Data Management</p>

    </body>
    </html>