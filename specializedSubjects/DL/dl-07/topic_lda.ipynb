{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample program for Topic Model Analysis using gensim (LDA)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_numeric\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_in = 'newsgroups5-1.csv'\n",
    "\n",
    "model_file = 'topic_newsgroups5-1.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_in, delimiter=',', skiprows=0, header=0)\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of documents in each category  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['target'].value_counts())\n",
    "n_targets = df['target'].value_counts().shape[0]\n",
    "print(n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign docID according to its category  \n",
    "- docID = 'd' + number, such as d0, d1, ..., d1000, d1001, ...\n",
    " - number = target * 1000 + j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docID = []\n",
    "j = np.zeros(n_targets)\n",
    "for i in range(len(df)):\n",
    "    tgt = df.at[i, 'target']\n",
    "    # base of document ID:\n",
    "    #   0 for documents of target 0, 1000 for documents of target 1,\n",
    "    #   2000 for documents of target 2, ...\n",
    "    docID.append('d'+str(int(tgt*1000+j[tgt])))\n",
    "    # increment j for target \"tgt\"\n",
    "    j[tgt] += 1\n",
    "df['docID'] = docID\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop_words, punctuations, etc.   \n",
    "- Use custom filters  \n",
    "- in this case, strip_multiple_whiltespaces is not used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [lambda x: x.lower(),\n",
    "                  strip_tags, remove_stopwords,\n",
    "                  strip_punctuation, strip_numeric, \n",
    "                 ]\n",
    "df['content'] = df['content'].map(lambda y: preprocess_string(y, CUSTOM_FILTERS))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-significant words are omitted.  \n",
    "Now the value of \"content\" column is a list of words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary(df['content'])\n",
    "print('#docs:', dic.num_docs)\n",
    "print('#vocabulary_size:', len(dic))\n",
    "print('#words_in_total:', dic.num_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check frequency of each word in corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = pd.DataFrame([])\n",
    "df_dic['word_id'] = list(dic.cfs.keys())\n",
    "df_dic['word'] = df_dic['word_id'].map(dic)\n",
    "df_dic['frequency'] = list(dic.cfs.values())\n",
    "df_dic_sorted = df_dic.sort_values('frequency', ascending=False)\n",
    "display(df_dic_sorted.head())\n",
    "display(df_dic_sorted.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter word set (omit too less / too much frequent words)  \n",
    "- no_below: Minimum frequency of a word for the analysis  \n",
    "- no_above: Maximum ratio of appearance in documents (omit words appeared in too many docs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.filter_extremes(no_below=5, no_above=0.1)\n",
    "print('#docs:', dic.num_docs)\n",
    "print('#vocabulary_size:', len(dic))\n",
    "print('#words_in_total:', dic.num_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After the filtering, the vocabulary size is shrinked**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check frequency of each word in corpus after filtering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = pd.DataFrame([],)\n",
    "df_dic['word_id'] = list(dic.cfs.keys())\n",
    "df_dic['word'] = df_dic['word_id'].map(dic)\n",
    "df_dic['frequency'] = list(dic.cfs.values())\n",
    "df_dic_sorted = df_dic.sort_values('frequency', ascending=False)\n",
    "display(df_dic_sorted.head())\n",
    "display(df_dic_sorted.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make corpus (BoW for each document)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dic.doc2bow(text) for text in df['content']]\n",
    "print('corpus:')\n",
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of LDA  \n",
    "- Set n_topics = 5 for example (same as the number of newsgroups)  \n",
    "- Set alpha = 'symmetric', i.e. prior probability for each topic is uniform      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "alpha = 'symmetric'\n",
    "lda = LdaModel(corpus, num_topics=n_topics, alpha=alpha, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic probability for each doc  \n",
    "- A list of (topic_id, topic_probability)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show topic prob for the 1st 7 documents as samples\n",
    "for c in corpus[:7]:\n",
    "    doc_topics = lda.get_document_topics(c)\n",
    "    print(doc_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correspondence between category (newsgroup) and topics  \n",
    "- Average probability of topics of documents for each category  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_topics = np.zeros((n_targets, n_topics))\n",
    "n_tgt_docs = np.zeros(n_targets)\n",
    "tgt_names = ['']*n_targets\n",
    "for i in range(dic.num_docs):\n",
    "    target_id = df.at[i, 'target']\n",
    "    if tgt_names[target_id] == '':\n",
    "        tgt_names[target_id] = df.at[i, 'target_names']\n",
    "    n_tgt_docs[target_id] += 1\n",
    "    for topic_id,topic_prob in lda.get_document_topics(corpus[i]):\n",
    "        cat_topics[target_id, topic_id] += topic_prob\n",
    "\n",
    "for i in range(n_targets):\n",
    "    cat_topics[i, :] /= n_tgt_docs[i]\n",
    "\n",
    "df_cat_topics = pd.DataFrame(cat_topics)\n",
    "df_cat_topics['target_names'] = tgt_names\n",
    "display(df_cat_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word probability for each topic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 5\n",
    "for i in range(n_topics):\n",
    "    print('Topic #{}'.format(i))\n",
    "    for word_id,prob in lda.get_topic_terms(i, topn=n_top):\n",
    "        print(dic[word_id], prob)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of word probability using wordcloud  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    colormap=\"winter\",\n",
    "    collocations=False,\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(6,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.3)\n",
    "for i in range(n_topics):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    x = {dic[t[0]]:t[1] for t in lda.get_topic_terms(i, topn=20)}\n",
    "    im = wc.generate_from_frequencies(x)\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.title('Topic #'+str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that correspondence between categories (newsgroups) and topics exists.  \n",
    "Note that probability of topics is determined per document, not per newsgroup, and various subjects are included in articles of a newsgroup (e.g. NOT all articles in rec.sports.baseball talk about baseball)**  \n",
    "\n",
    "**Target 0 (rec.autos): mainly generated from topics #1 and #3.  \n",
    "Target 1 (soc.religion.christian): mainly generated from topic #4.  \n",
    "Target 2 (rec.sport.baseball): mainly generated from topic #0.  \n",
    "Target 3 (sci.med): mainly generated from topics #2 and #3.  \n",
    "Target 4 (sci.electronics): mainly generated from topics #1 and #4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
