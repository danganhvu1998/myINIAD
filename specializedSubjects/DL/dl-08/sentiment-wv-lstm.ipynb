{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample program for LSTM (text input/class output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_in = 'umich_si650_half.txt'\n",
    "model_file = 'word2vec_text8-min20-s300-nosim.model'\n",
    "\n",
    "# To show more rows and columns without \"...\"\n",
    "pd.options.display.max_columns=999\n",
    "pd.options.display.max_rows=999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(csv_in, delimiter='\\t', header=None)\n",
    "df.columns = ['label', 'text']\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and preprocess data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(preprocess_string)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "y = to_categorical(y)\n",
    "y_ndim = y.shape[1]\n",
    "print(y_ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.head())\n",
    "print(y_train[:3])\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(X_test.head())\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv_model.wv.vector_size)  # dimension of embedding\n",
    "print(len(wv_model.wv.vocab.keys()))  # number of words\n",
    "print(list(wv_model.wv.vocab.keys())[:10])  # show first 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add text of train data to vocab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v = X_train.values\n",
    "print(X_train_v.shape)\n",
    "\n",
    "n_words = 0\n",
    "for i in range(len(X_train_v)):\n",
    "    n_words += len(X_train_v[i])\n",
    "\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model.build_vocab(X_train_v, update=True)\n",
    "wv_model.train(X_train_v, total_examples=len(X_train_v),\n",
    "               total_words=n_words, epochs=wv_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wv_model.wv.vocab.keys()))  # number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add word_index to vocabularies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "texts = ' '.join(wv_model.wv.vocab.keys())\n",
    "tokenizer.fit_on_texts([texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "emb_size = wv_model.wv.vector_size\n",
    "print(len(word_index.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make word2vec vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = len(word_index)\n",
    "emb = np.zeros((nw+1, emb_size))\n",
    "for w, i in word_index.items():\n",
    "    emb[i] = wv_model[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for train data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = X_train.map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq= tokenizer.texts_to_sequences(X_train_v)\n",
    "print(len(seq))\n",
    "print(seq[:3])\n",
    "print(X_train_v[:3])\n",
    "seq_pad = pad_sequences(seq, maxlen=max_len)\n",
    "print(len(seq_pad))\n",
    "print(seq_pad[:3])\n",
    "seq_ar = np.array(seq_pad)\n",
    "print(seq_ar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 20\n",
    "n_out = y_ndim\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nw+1, emb_size, weights=[emb],\n",
    "                    input_length=max_len, mask_zero=True,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(n_hidden, return_sequences=False))\n",
    "model.add(Dense(n_out, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (learning)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "val_split=0.2\n",
    "model.fit(seq_ar, y_train,\n",
    "          epochs=n_epochs, batch_size=batch_size,\n",
    "          validation_split=val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test=tokenizer.texts_to_sequences(X_test.values)\n",
    "seq_test_pad = pad_sequences(seq_test, maxlen=max_len)\n",
    "seq_test_ar = np.array(seq_test_pad)\n",
    "print(seq_test_ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(seq_test_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = y_pred.argmax(axis=1)\n",
    "print(y_pred.shape)\n",
    "print(y_pred1.shape)\n",
    "\n",
    "y_test1 = y_test.argmax(axis=1)\n",
    "print(y_test.shape)\n",
    "print(y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(y_pred1,y_test1))\n",
    "print(accuracy_score(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
